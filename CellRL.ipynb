{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91b9707d-5c4a-4318-bf25-0ea37748dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: matplotlib in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (6.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (3.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (4.28.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib) (58.0.4)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib) (1.2.2)\n",
      "Requirement already satisfied: numpy in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras\n",
    "!pip3 install matplotlib\n",
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d64618e-0087-4fea-8a4d-006796fc0e03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12124/1429048665.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl_env/lib/python3.9/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "REPLAY_MEMORY_SIZE = 50_000\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000\n",
    "MODEL_NAME = \"256x2\"\n",
    "\n",
    "\n",
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "\n",
    "        # main model  # gets trained every step\n",
    "        self.model = self.create_model()\n",
    "\n",
    "        # Target model this is what we .predict against every step\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{MODEL_NAME}-{int(time.time())}\")\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(256, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64))\n",
    "\n",
    "        model.add(Dense(env.ACTION_SPACE_SIZE, activiation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def get_qs(self, state, step):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "    # Trains main network every step during episode\n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ee9fd36-3799-4d4a-aa61-37791ed885ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "61.59559064080081\n",
      "2\n",
      "61.21681931325304\n",
      "3\n",
      "60.838047985705266\n",
      "4\n",
      "60.524411557617185\n",
      "5\n",
      "60.271269163504385\n",
      "6\n",
      "60.104782616778024\n",
      "7\n",
      "59.93829607005166\n",
      "8\n",
      "59.7718095233253\n",
      "9\n",
      "59.7521344636885\n",
      "20\n",
      "59.62726196701227\n",
      "21\n",
      "59.583833518657386\n",
      "37\n",
      "59.39527765373977\n",
      "38\n",
      "59.29979726854451\n",
      "39\n",
      "59.20431688334925\n",
      "46\n",
      "59.202968612576996\n",
      "[0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      "episode reward: 15\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class CellEnv:\n",
    "    observation_method = 1 # 0 for state value, 1 for error value\n",
    "    grid_SIZE = 10.0\n",
    "    Nb_robot = 10\n",
    "    RETURN_IMAGES = False\n",
    "    IMPROVE_REWARD = 1\n",
    "    GOAL_REWARD = 30\n",
    "    velocity = 0.1\n",
    "    # ENEMY_PENALTY = 300 \n",
    "    # FOOD_REWARD = 25\n",
    "    # OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)  # 4\n",
    "    #ACTION_SPACE_SIZE = 9\n",
    "    \n",
    "    # the dict! (colors)\n",
    "    d = {1: (255, 175, 0),\n",
    "         2: (0, 255, 0),\n",
    "         3: (0, 0, 255)}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.reach_goal_flag = np.zeros((1, self.Nb_robot), dtype=bool)\n",
    "        self.reach_goal_count = 0\n",
    "        self.goal = goal(self.grid_SIZE)\n",
    "        self.robots = []\n",
    "        for i in range(self.Nb_robot):\n",
    "            self.robots.append(robot(self.grid_SIZE,self.velocity))\n",
    "            error = [self.goal.x-self.robots[i].x, self.goal.y-self.robots[i].y]\n",
    "            while  np.linalg.norm(error, ord=2) < 0.1:\n",
    "                self.robots[i] = robot(self.grid_SIZE,self.velocity)\n",
    "                error = [self.goal.x-self.robots[i].x, self.goal.y-self.robots[i].y]\n",
    "        self.episode_step = 0\n",
    "        # if self.RETURN_IMAGES:\n",
    "        #     #observation = np.array(self.get_image())\n",
    "        # else:\n",
    "        observation = []\n",
    "        error = []\n",
    "        self.target_state = [self.goal.x,self.goal.y,np.array([0.0])]\n",
    "        for robot_i in self.robots:\n",
    "            theta_reference_i = np.arctan2(self.goal.y-robot_i.y, self.goal.x-robot_i.x)\n",
    "            error_i = [self.goal.x-robot_i.x, self.goal.y-robot_i.y, theta_reference_i - robot_i.theta]\n",
    "            if self.observation_method == 0:\n",
    "                observation_i = [robot_i.x, robot_i.y, robot_i.theta]\n",
    "            elif self.observation_method == 1:\n",
    "                observation_i = error_i\n",
    "            observation.append(observation_i) \n",
    "            error.append(error_i)\n",
    "        observation.append(self.target_state)\n",
    "        observation = np.concatenate(observation,axis=1)\n",
    "        observation = observation.reshape(1,3,-1)\n",
    "        error = np.concatenate(error,axis=1)\n",
    "        error = error.reshape(1,3,-1)\n",
    "        self.error_sum_min = sum(sum(abs(error[0,0:2,:])))\n",
    "        self.error_sum_min_step = 0\n",
    "        # print(type(observation))\n",
    "        # print(observation.shape)\n",
    "        self.last_observation = observation\n",
    "        self.last_error = error\n",
    "        return observation,self.goal,self.observation_method\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.reach_goal_flag = np.zeros((1, self.Nb_robot), dtype=bool)\n",
    "        self.episode_step += 1\n",
    "        reward = 0\n",
    "        # if self.RETURN_IMAGES:\n",
    "        #     # new_observation = np.array(self.get_image())\n",
    "        # else:\n",
    "        for robot_i in self.robots:\n",
    "            robot_i.follow_action(action)\n",
    "        theta_reference = []\n",
    "        error = []\n",
    "        new_observation = []\n",
    "        for robot_i in self.robots:\n",
    "            theta_reference_i = np.arctan2(self.goal.y-robot_i.y, self.goal.x-robot_i.x)\n",
    "            error_i = [self.goal.x-robot_i.x, self.goal.y-robot_i.y, theta_reference_i - robot_i.theta]\n",
    "            if self.observation_method == 0:\n",
    "                observation_i = [robot_i.x, robot_i.y, robot_i.theta]\n",
    "            elif self.observation_method == 1:\n",
    "                observation_i = error_i\n",
    "            theta_reference.append(theta_reference_i)\n",
    "            new_observation.append(observation_i)\n",
    "            error.append(error_i)\n",
    "        new_observation.append(self.target_state)\n",
    "        new_observation = np.concatenate(new_observation,axis=1)\n",
    "        new_observation = new_observation.reshape(1,3,-1)\n",
    "        error = np.concatenate(error,axis=1)\n",
    "        error = error.reshape(1,3,-1)\n",
    "\n",
    "        done = False\n",
    "        i = 0\n",
    "        for robot_i in self.robots:\n",
    "            if abs(robot_i.x - self.goal.x)<3e-1 and abs(robot_i.y - self.goal.y)<3e-1:\n",
    "                self.reach_goal_flag[0,i] = True\n",
    "            i = i+1\n",
    "        #if sum(sum(np.abs(error[0,0:2,:])-np.abs(self.last_error[0,0:2,:]))) > 0.4:  \n",
    "        if sum(sum(error[0,0:2,:]-self.last_error[0,0:2,:])) > 0.5:  \n",
    "            reward += self.IMPROVE_REWARD\n",
    "        if np.count_nonzero(self.reach_goal_flag)>self.reach_goal_count:\n",
    "            self.reach_goal_count=np.count_nonzero(self.reach_goal_flag)\n",
    "            reward += self.GOAL_REWARD\n",
    "        if  self.episode_step >= 200: \n",
    "            done = True\n",
    "        if self.error_sum_min > sum(sum(abs(error[0,0:2,:]))):\n",
    "            self.error_sum_min = sum(sum(abs(error[0,0:2,:])))\n",
    "            self.error_sum_min_step = self.episode_step\n",
    "            # print(self.episode_step)\n",
    "            # print(self.error_sum_min)\n",
    "        if self.episode_step - self.error_sum_min_step > 20:\n",
    "            done = True\n",
    "        self.last_observation = new_observation[:][:][:]\n",
    "        self.last_error = error[:][:][:]\n",
    "        return new_observation, reward, done\n",
    "\n",
    "    # def render(self):\n",
    "        # img = self.get_image()\n",
    "        # img = img.resize((300, 300))  # resizing so we can see our robot in all its glory.\n",
    "        # cv2.imshow(\"image\", np.array(img))  # show it!\n",
    "        # cv2.waitKey(1)\n",
    "\n",
    "class goal:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.uniform(0.1*size, 0.9*size, 1) # dont be too close to the boundary\n",
    "        self.y = np.random.uniform(0.1*size, 0.9*size, 1) ##np.random.random(0.1*size, 0.9*size)\n",
    "\n",
    "class robot:\n",
    "    def __init__(self, size,velocity):\n",
    "        self.size = size\n",
    "        self.velocity = velocity\n",
    "        self.x = np.random.uniform(0.05*size, 0.95*size, 1) \n",
    "        self.y = np.random.uniform(0.05*size, 0.95*size, 1) \n",
    "        self.theta = np.random.uniform(-np.pi,np.pi,1)\n",
    "        self.mu, self.sigma =  np.pi/4, np.pi/180*5 # mean and standard deviation\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"mi-robot ({self.x}, {self.y},{self.theta})\"\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y, self.theta-other.theta)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def follow_action(self, choice):\n",
    "        if choice == 1:\n",
    "            delta_theta = np.random.normal(self.mu, self.sigma)\n",
    "            self.theta = self.theta + delta_theta\n",
    "            if self.theta > np.pi:\n",
    "                self.theta -= 2*np.pi\n",
    "            elif self.theta < -np.pi:\n",
    "                self.theta += 2*np.pi\n",
    "        elif choice == 0:\n",
    "            self.theta = self.theta\n",
    "        self.move(xx=self.velocity*np.cos(self.theta), yy=self.velocity*np.sin(self.theta))\n",
    "\n",
    "    def move(self, xx=False, yy=False):\n",
    "        # If no value for x, move randomly\n",
    "        if not xx:\n",
    "            print(\"no x\")# self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += xx\n",
    "        # If no value for y, move randomly\n",
    "        if not yy:\n",
    "            print(\"no y\")# self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += yy\n",
    "        # If we are out of bounds, fix!\n",
    "        if self.x < 0:\n",
    "            self.x = np.array([0.])\n",
    "        elif self.x > self.size:\n",
    "            self.x = np.array([self.size])\n",
    "        if self.y < 0:\n",
    "            self.y = np.array([0.])\n",
    "        elif self.y > self.size:\n",
    "            self.y = np.array([self.size])\n",
    "class goal:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.uniform(0.1*size, 0.9*size, 1) # dont be too close to the boundary\n",
    "        self.y = np.random.uniform(0.1*size, 0.9*size, 1) ##np.random.random(0.1*size, 0.9*size)\n",
    "        \n",
    "env = CellEnv()\n",
    "\n",
    "# # For stats\n",
    "# ep_rewards = [-200]\n",
    "# For more repetitive results\n",
    "# random.seed(1)\n",
    "# tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# import os\n",
    "# # Create models folder\n",
    "# if not os.path.isdir('models'):\n",
    "#     os.makedirs('models')\n",
    "episode_reward = 0\n",
    "step = 1\n",
    "# Reset environment and get initial state\n",
    "current_state,goal,observation_method = env.reset()\n",
    "episode_reward = 0\n",
    "epsilon = 1\n",
    "#After restarting the episode and state, we're ready to iterate over the steps per episode:\n",
    "# Reset flag and start iterating until episode ends\n",
    "all_state_buffer = [current_state[:][:][:]];\n",
    "all_reward_buffer = [];\n",
    "all_action_buffer = [];\n",
    "done = False\n",
    "while not done:\n",
    "    # This part stays mostly the same, the change is to query a model for Q values\n",
    "    if np.random.random() > epsilon:\n",
    "        # Get action from Q table\n",
    "        #action = np.argmax(agent.get_qs(current_state))\n",
    "        action = np.random.randint(0, 2)\n",
    "    else:\n",
    "        # Get random action\n",
    "        action = np.random.randint(0, 2)\n",
    "    new_state, reward, done = env.step(action)\n",
    "    episode_reward += reward\n",
    "    current_state = new_state[:][:][:]\n",
    "    step += 1\n",
    "    all_state_buffer.append(current_state[:][:][:]);\n",
    "    all_reward_buffer.append(reward)\n",
    "    all_action_buffer.append(action)\n",
    "all_reward_buffer = np.array(all_reward_buffer)\n",
    "print(all_reward_buffer)\n",
    "# print(all_reward_buffer.shape)\n",
    "# all_reward_buffer = np.concatenate(all_reward_buffer,axis=0)\n",
    "# print(all_reward_buffer)\n",
    "print(\"episode reward:\",episode_reward)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "grid_SIZE = 10\n",
    "Nb_agent = 10\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([goal.x],[goal.y],'go')\n",
    "ax.axis([0,grid_SIZE,0,grid_SIZE])\n",
    "l, = ax.plot([],[],'ro')\n",
    "\n",
    "def animate(i):\n",
    "    if i > 0:\n",
    "        del fig.texts[0:len(fig.texts)]\n",
    "    ax.plot([goal.x],[goal.y],'go')\n",
    "    if observation_method == 0:\n",
    "        l.set_data(all_state_buffer[i][0,0,:], all_state_buffer[i][0,1,:])\n",
    "    elif observation_method == 1:\n",
    "        l.set_data(goal.x-all_state_buffer[i][0,0,:], goal.y-all_state_buffer[i][0,1,:])\n",
    "    fig.text(0.1, 0.9, 'Reward:'+str(all_reward_buffer[i-1]), size=10, color='purple')\n",
    "    fig.text(0.5, 0.9, 'action:'+str(all_action_buffer[i-1]), size=10, color='purple')\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(all_state_buffer) ) #len(all_state_buffer)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())\n",
    "f = r\"cell_movement_animation.gif\" \n",
    "writergif = animation.PillowWriter(fps=2) \n",
    "ani.save(f, writer=writergif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3dcb1a-57c4-42b8-9bba-40296e1958d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "\n",
    "t = np.linspace(0,2*np.pi)\n",
    "x = np.sin(t)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis([0,2*np.pi,-1,1])\n",
    "l, = ax.plot([],[])\n",
    "\n",
    "def animate(i):\n",
    "    l.set_data(t[:i], x[:i])\n",
    "\n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=len(t))\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf6a49-777e-4ed3-983a-2439fadd63dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(random.randrange(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941aed4-80c5-4071-8edd-0e549ddb1039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False False False  True False False False False]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr_2d = np.zeros((1, 10), dtype=bool)\n",
    "arr_2d[0,5]=True\n",
    "print(arr_2d)\n",
    "np.count_nonzero(arr_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021cee27-b2b5-4ded-9466-3e58ef154ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342305d-4be7-4bc0-a672-20e34305628d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

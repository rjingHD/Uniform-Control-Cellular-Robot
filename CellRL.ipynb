{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91b9707d-5c4a-4318-bf25-0ea37748dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: matplotlib in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (6.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (3.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from matplotlib) (4.28.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib) (58.0.4)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib) (1.2.2)\n",
      "Requirement already satisfied: numpy in /home/ranjing/anaconda3/envs/rl_env/lib/python3.9/site-packages (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras\n",
    "!pip3 install matplotlib\n",
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d64618e-0087-4fea-8a4d-006796fc0e03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12124/1429048665.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl_env/lib/python3.9/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "REPLAY_MEMORY_SIZE = 50_000\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000\n",
    "MODEL_NAME = \"256x2\"\n",
    "\n",
    "\n",
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "\n",
    "        # main model  # gets trained every step\n",
    "        self.model = self.create_model()\n",
    "\n",
    "        # Target model this is what we .predict against every step\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{MODEL_NAME}-{int(time.time())}\")\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(256, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64))\n",
    "\n",
    "        model.add(Dense(env.ACTION_SPACE_SIZE, activiation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def get_qs(self, state, step):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "    # Trains main network every step during episode\n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ee9fd36-3799-4d4a-aa61-37791ed885ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 1\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 1\n",
      "reward: 1\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 1\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 1\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "reward: 0\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(200,)\n",
      "episode reward: 23\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class CellEnv:\n",
    "    grid_SIZE = 10.0\n",
    "    Nb_robot = 10\n",
    "    RETURN_IMAGES = False\n",
    "    IMPROVE_REWARD = 1\n",
    "    GOAL_REWARD = 30\n",
    "    velocity = 0.1\n",
    "    # ENEMY_PENALTY = 300 \n",
    "    # FOOD_REWARD = 25\n",
    "    # OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)  # 4\n",
    "    #ACTION_SPACE_SIZE = 9\n",
    "    \n",
    "    # the dict! (colors)\n",
    "    d = {1: (255, 175, 0),\n",
    "         2: (0, 255, 0),\n",
    "         3: (0, 0, 255)}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.reach_goal_flag = np.zeros((1, self.Nb_robot), dtype=bool)\n",
    "        self.reach_goal_count = 0\n",
    "        self.goal = goal(self.grid_SIZE)\n",
    "        self.robots = []\n",
    "        for i in range(self.Nb_robot):\n",
    "            self.robots.append(robot(self.grid_SIZE,self.velocity))\n",
    "            error = [self.goal.x-self.robots[i].x, self.goal.y-self.robots[i].y]\n",
    "            while  np.linalg.norm(error, ord=2) < 0.1:\n",
    "                self.robots[i] = robot(self.grid_SIZE,self.velocity)\n",
    "                error = [self.goal.x-self.robots[i].x, self.goal.y-self.robots[i].y]\n",
    "        self.episode_step = 0\n",
    "        # if self.RETURN_IMAGES:\n",
    "        #     #observation = np.array(self.get_image())\n",
    "        # else:\n",
    "        observation = []\n",
    "        error = []\n",
    "        self.target_state = [self.goal.x,self.goal.y,np.array([0.0])]\n",
    "        for robot_i in self.robots:\n",
    "            theta_reference_i = np.arctan2(self.goal.y-robot_i.y, self.goal.x-robot_i.x)\n",
    "            error_i = [self.goal.x-robot_i.x, self.goal.y-robot_i.y, theta_reference_i - robot_i.theta]\n",
    "            observation_i = [robot_i.x, robot_i.y, robot_i.theta]\n",
    "            observation.append(observation_i) \n",
    "            error.append(error_i)\n",
    "        observation.append(self.target_state)\n",
    "        observation = np.concatenate(observation,axis=1)\n",
    "        observation = observation.reshape(1,3,-1)\n",
    "        error = np.concatenate(error,axis=1)\n",
    "        error = error.reshape(1,3,-1)\n",
    "        # print(type(observation))\n",
    "        # print(observation.shape)\n",
    "        self.last_observation = observation\n",
    "        self.last_error = error\n",
    "        return observation,self.goal\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.reach_goal_flag = np.zeros((1, self.Nb_robot), dtype=bool)\n",
    "        self.episode_step += 1\n",
    "        reward = 0\n",
    "        # if self.RETURN_IMAGES:\n",
    "        #     # new_observation = np.array(self.get_image())\n",
    "        # else:\n",
    "        for robot_i in self.robots:\n",
    "            robot_i.follow_action(action)\n",
    "        theta_reference = []\n",
    "        error = []\n",
    "        new_observation = []\n",
    "        for robot_i in self.robots:\n",
    "            theta_reference_i = np.arctan2(self.goal.y-robot_i.y, self.goal.x-robot_i.x)\n",
    "            error_i = [self.goal.x-robot_i.x, self.goal.y-robot_i.y, theta_reference_i - robot_i.theta]\n",
    "            observation_i = [robot_i.x, robot_i.y, robot_i.theta]\n",
    "            theta_reference.append(theta_reference_i)\n",
    "            new_observation.append(observation_i)\n",
    "            error.append(error_i)\n",
    "        new_observation.append(self.target_state)\n",
    "        new_observation = np.concatenate(new_observation,axis=1)\n",
    "        new_observation = new_observation.reshape(1,3,-1)\n",
    "        error = np.concatenate(error,axis=1)\n",
    "        error = error.reshape(1,3,-1)\n",
    "\n",
    "        done = False\n",
    "        \n",
    "        i = 0\n",
    "        for robot_i in self.robots:\n",
    "            if abs(robot_i.x - self.goal.x)<3e-1 and abs(robot_i.y - self.goal.y)<3e-1:\n",
    "                self.reach_goal_flag[0,i] = True\n",
    "            i = i+1\n",
    "        if sum(sum(np.abs(error[0,0:2,:])-np.abs(self.last_error[0,0:2,:]))) > 0.4:  \n",
    "            reward += self.IMPROVE_REWARD\n",
    "        if np.count_nonzero(self.reach_goal_flag)>self.reach_goal_count:\n",
    "            self.reach_goal_count=np.count_nonzero(self.reach_goal_flag)\n",
    "            reward += self.GOAL_REWARD\n",
    "        if  self.episode_step >= 200:  #reward == self.FOOD_REWARD or reward == -self.ENEMY_PENALTY or\n",
    "            done = True\n",
    "        self.last_observation = new_observation[:][:][:]\n",
    "        self.last_error = error[:][:][:]\n",
    "        return new_observation, reward, done\n",
    "\n",
    "    # def render(self):\n",
    "        # img = self.get_image()\n",
    "        # img = img.resize((300, 300))  # resizing so we can see our robot in all its glory.\n",
    "        # cv2.imshow(\"image\", np.array(img))  # show it!\n",
    "        # cv2.waitKey(1)\n",
    "\n",
    "class goal:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.uniform(0.1*size, 0.9*size, 1) # dont be too close to the boundary\n",
    "        self.y = np.random.uniform(0.1*size, 0.9*size, 1) ##np.random.random(0.1*size, 0.9*size)\n",
    "\n",
    "class robot:\n",
    "    def __init__(self, size,velocity):\n",
    "        self.size = size\n",
    "        self.velocity = velocity\n",
    "        self.x = np.random.uniform(0.05*size, 0.95*size, 1) \n",
    "        self.y = np.random.uniform(0.05*size, 0.95*size, 1) \n",
    "        self.theta = np.random.uniform(-np.pi,np.pi,1)\n",
    "        self.mu, self.sigma =  np.pi/4, np.pi/180*5 # mean and standard deviation\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"mi-robot ({self.x}, {self.y},{self.theta})\"\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y, self.theta-other.theta)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def follow_action(self, choice):\n",
    "        if choice == 1:\n",
    "            delta_theta = np.random.normal(self.mu, self.sigma)\n",
    "            self.theta = self.theta + delta_theta\n",
    "            if self.theta > np.pi:\n",
    "                self.theta -= 2*np.pi\n",
    "            elif self.theta < -np.pi:\n",
    "                self.theta += 2*np.pi\n",
    "        elif choice == 0:\n",
    "            self.theta = self.theta\n",
    "        self.move(xx=self.velocity*np.cos(self.theta), yy=self.velocity*np.sin(self.theta))\n",
    "\n",
    "    def move(self, xx=False, yy=False):\n",
    "\n",
    "        # If no value for x, move randomly\n",
    "        if not xx:\n",
    "            print(\"no x\")# self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += xx\n",
    "\n",
    "        # If no value for y, move randomly\n",
    "        if not yy:\n",
    "            print(\"no y\")# self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += yy\n",
    "\n",
    "        # If we are out of bounds, fix!\n",
    "        if self.x < 0:\n",
    "            self.x = np.array([0.])\n",
    "            #print(\"xlow\")\n",
    "        elif self.x > self.size:\n",
    "            self.x = np.array([self.size])\n",
    "            #print(\"xhigh\")\n",
    "        if self.y < 0:\n",
    "            self.y = np.array([0.])\n",
    "            #print(\"ylow\")\n",
    "        elif self.y > self.size:\n",
    "            self.y = np.array([self.size])\n",
    "            #print(\"yhigh\")\n",
    "\n",
    "class goal:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.uniform(0.1*size, 0.9*size, 1) # dont be too close to the boundary\n",
    "        self.y = np.random.uniform(0.1*size, 0.9*size, 1) ##np.random.random(0.1*size, 0.9*size)\n",
    "        \n",
    "env = CellEnv()\n",
    "\n",
    "# # For stats\n",
    "# ep_rewards = [-200]\n",
    "# For more repetitive results\n",
    "# random.seed(1)\n",
    "# tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# import os\n",
    "# # Create models folder\n",
    "# if not os.path.isdir('models'):\n",
    "#     os.makedirs('models')\n",
    "episode_reward = 0\n",
    "step = 1\n",
    "# Reset environment and get initial state\n",
    "current_state,goal = env.reset()\n",
    "episode_reward = 0\n",
    "epsilon = 1\n",
    "#After restarting the episode and state, we're ready to iterate over the steps per episode:\n",
    "# Reset flag and start iterating until episode ends\n",
    "all_state_buffer = [current_state[:][:][:]];\n",
    "all_reward_buffer = [];\n",
    "done = False\n",
    "while not done:\n",
    "    # This part stays mostly the same, the change is to query a model for Q values\n",
    "    if np.random.random() > epsilon:\n",
    "        # Get action from Q table\n",
    "        #action = np.argmax(agent.get_qs(current_state))\n",
    "        action = np.random.randint(0, 2)\n",
    "    else:\n",
    "        # Get random action\n",
    "        action = np.random.randint(0, 2)\n",
    "    new_state, reward, done = env.step(action)\n",
    "    episode_reward += reward\n",
    "    print(\"reward:\",reward)\n",
    "        # if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "        #     env.render()\n",
    "        # Every step we update replay memory and train main network\n",
    "        #agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        #agent.train(done, step)\n",
    "    current_state = new_state[:][:][:]\n",
    "    step += 1\n",
    "    all_state_buffer.append(current_state[:][:][:]);\n",
    "    all_reward_buffer.append(reward)\n",
    "all_reward_buffer = np.array(all_reward_buffer)\n",
    "print(all_reward_buffer)\n",
    "print(all_reward_buffer.shape)\n",
    "# all_reward_buffer = np.concatenate(all_reward_buffer,axis=0)\n",
    "# print(all_reward_buffer)\n",
    "\n",
    "print(\"episode reward:\",episode_reward)\n",
    "\n",
    "\n",
    "# print(np.array(all_state_buffer[1])[:,1])\n",
    "# print(goal.x-all_state_buffer[1][1,:])\n",
    "# print(goal.y-all_state_buffer[1][2,:])\n",
    "# print(all_state_buffer[10][1,:])\n",
    "# for i in range(len(all_state_buffer)):\n",
    "#     print(all_state_buffer[i][2,:])\n",
    "#     print(all_state_buffer[i][2,:]<0)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "grid_SIZE = 10\n",
    "Nb_agent = 10\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([goal.x],[goal.y],'go')\n",
    "ax.axis([0,grid_SIZE,0,grid_SIZE])\n",
    "l, = ax.plot([],[],'ro')\n",
    "\n",
    "def animate(i):\n",
    "    if i > 0:\n",
    "        del fig.texts[0]\n",
    "    ax.plot([goal.x],[goal.y],'go')\n",
    "    l.set_data(all_state_buffer[i][0,0,:], all_state_buffer[i][0,1,:])\n",
    "    fig.text(0.1, 0.9, 'Step Reward:'+str(all_reward_buffer[i-1]), size=15, color='purple')\n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=100 ) #len(all_state_buffer)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())\n",
    "f = r\"cell_movement_animation.gif\" \n",
    "writergif = animation.PillowWriter(fps=5) \n",
    "ani.save(f, writer=writergif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3dcb1a-57c4-42b8-9bba-40296e1958d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "\n",
    "t = np.linspace(0,2*np.pi)\n",
    "x = np.sin(t)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis([0,2*np.pi,-1,1])\n",
    "l, = ax.plot([],[])\n",
    "\n",
    "def animate(i):\n",
    "    l.set_data(t[:i], x[:i])\n",
    "\n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=len(t))\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf6a49-777e-4ed3-983a-2439fadd63dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(random.randrange(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941aed4-80c5-4071-8edd-0e549ddb1039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False False False  True False False False False]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr_2d = np.zeros((1, 10), dtype=bool)\n",
    "arr_2d[0,5]=True\n",
    "print(arr_2d)\n",
    "np.count_nonzero(arr_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021cee27-b2b5-4ded-9466-3e58ef154ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342305d-4be7-4bc0-a672-20e34305628d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
